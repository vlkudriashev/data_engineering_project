## Общая информация о проекте
   Проект предусматривает разработку прототипа системы автоматизированного управления данными, которые будут собираться и храниться интернет-магазином. Нам предоставлены архивные данные о клиентах и транзакциях в CSV формате, которые необходимо внести в целевую СУБД. Проект включает разработку базы данных на основе исходных данных, развертывание ClickHouse для обработки данных и создание автоматизированного процесса с использованием Airflow. 

## Этапы решения задачи

* Разработать структуры баз данных на основе имеющихся архивных данных.
* Импортировать данные в OLTP систему управления базами данных (СУБД) Postgres.
* Развернуть СУБД ClickHouse для обработки данных, их длительного хранения, построения витрин данных.
* Создать и протестировать автоматизированный процесс (Pipeline) для обработки и сохранения новых поступающих данных при помощи инструмента оркестрации Airflow.
* Проверить работоспособность Pipeline со сторонними API. На Flask API необходимо разработать тестовый API для получения данных о кредитном рейтинге продавцов e-commerce площадки.

## Используемые инструменты
PostgreSQL, ClickHouse, Airflow, Pandas, Flask, Docker-Compose.
    
## Структура проекта
В папке "csv_files" находятся исходные исторические данные. Папки с префиксами "app_" содержат Docker-файлы и скрипты для создания и инициализации приложений с использованием Docker-Compose. Папка "dags" содержит исходный код Airlow DAGs и исполняемые Python скрипты. Файл .airflowignore необходим для исключения скриптов Python, отличных от DAGs, из планировщика Airflow.

## Настройка проекта

 * Клонировать репозиторий.
 
 * Произвести первичную настройку Airflow:
    https://airflow.apache.org/docs/apache-airflow/stable/howto/docker-compose/index.html
    Выполнить только пункты, указанные в "Setting the right Airflow user". Дальнейшая настройка будет производиться через Docker-Compose.
        
 * Из папки проекта, запустить следующие команды:
    ```
    docker-compose -f airflow_compose.yaml up airflow-init
    docker-compose -f airflow_compose.yaml -f db_compose.yaml up
    ```
    
## Описание схемы данных

* ### Source DB - Postgres.
	Файл «orders.csv» содержит данные о купленных товарах и их количестве. Значения этих полей представлены в виде массивов. Так как магазин планирует добавлять новые данные о заказах в OLTP-базу данных в нормализованном виде. Данные приводятся к нормализованному виду форматированием. Затем создаются отношения на их основе. Отношение "orders", содержащее массивы приводятся к третьей нормальной форме разбиением на «orders» и «order_items». Конечная схема:
 
<img src="https://github.com/vlkudriashev/data_engineering_project/blob/main/pics/csv_to_source.png" width=50%>
    

* ### Stage DB - Clickhouse
	В СУБД Clickhouse создаётся Staging слой для временного хранения и обработки данных, поступающих из различных внутренних и внешних источников. Такой слой позволяет не нагружать сервер исходной OLTP СУБД преобразованием данных, а также делает возможным проверку качества поступающих данных до переноса в основное хранилище.
    
* ### Target DB - Clickhouse
	Target слой создаётся для длительного хранения данных и дальнейшей работы с очищенными данными. На его основе могут создаваться витрины данных.
    
    Общая структура проекта:
    
<img src="https://github.com/vlkudriashev/data_engineering_project/blob/main/pics/project_structure.png" width=75%>
    
## Особенности реализации pipeline:
### Идемпотентность задач
Известно, что задачи по работе с данными могут выполниться некорректно при потери соединения с СУБД. Если часть данных всё же была доставлена в целевую систему, но задача не завершилась корректно, то при её повторном запуске в данных могут появиться дубликаты.

Поэтому при построении систем передачи данных применяется DELETE-WRITE паттерн. В нем для определенных временных интервалов данные удаляются, а затем записываются новые. Важно, чтобы обе операции выполнялись в одном ДАГ-таске, чтобы избежать дублирования записей в целевой СУБД в случае ошибок или перезапусков задач. 

Применение ALTER DELETE в ClickHouse может сказаться на стабильности системы управления базами данных. Частые удаления и вставки данных в Lightweight delete могут привести к непредвиденным последствиям. Для избежания дублирования данных при перезапуске задач используется DELETE-WRITE с разбиением на партиции. В Staging таблицах создаются дневные партиции, которые перезаписываются при перезапуске задач.

Также проблема дедуплицирования записей появляется в случаях, где требуется замена значений на более актуальные. Например, при обновлении данных пользователя в исходной системе необходимость перезаписать данные в целевой системе для пользователя с таким же ID. В традиционных СУБД эта проблема решается при помощи UPSERT (Update-Insert) операции. В таблицах Target слоя ClickHouse для этого используется движок ReplacingMergeTree. Детали решения проблемы производительности RMT таблиц и FINAL подробнее разобраны в пункте, где объясняется ввод данных в таблицу продуктов ["insert_tgt_products"](#задача-"insert_tgt_products"(SCD)).

### Возможность backfilling
Все ДАГи поддерживают возможность заполнения значений за прошедшие периоды времени. Это возможно благодаря тому, что в коде SQL-запросов используются динамические даты из jinja-шаблонов. 
       
## Описание ДАГов, детали выполнения проекта

### 0.1. Расписание задач Дагов
В большинстве дагов указан режим выполнения "@once" для облегчения их первичной отладки и проверки. Дата начала исполнения ДАГов указана как дата самого раннего события (действия пользователей) в исходных данных. Дата окончания исполнения ДАГов указана, чтобы избежать множественного исполнения дагов при тестировании. Режим выполнения ДАГов возможно изменить согласно необходимости.

### 0.2. Режим загрузки данных
В приведенном Pipeline тестируется возможность инкрементальной загрузки данных и возможность загрузки данных за определенный период времени. Не выполняется стадия полной загрузки данных в Stage и Target, хотя ДАГи легко модифицируются под эти цели.
  
 ### 1. CSV в Source (Postgres)
 
* Подготовка данных к импорту
 
   ДАГ №1 загружает данные из CSV-файлов в "source" схему СУБД PostgreSQL.  Файлы могут содержать пропуски идентификаторов, которые далее используются как первичные ключи таблиц (например, user_id, order_id). Внешние ключи в других таблицах могут ссылаться на пропущенные значения. Если пропуски не устранить, это может привести к нарушению ссылочной целостности и невозможности импорта данных. Используя Pandas, мы заполняем пропуски в порядковых значениях для первичных ключей в таблицах. Все данные, включая отсутствующие значения (NULL), импортируются из датафрейма Pandas и преобразуются в типы данных, поддерживаемые коннектором "Psycopg".
    
* Импорт данных
    
    В скрипте импорта данных "csv_to_oltp" создаётся одно соединение для импорта всех файлов. Запись производится методом "execute_batch", а сами данные вводятся при помощи итератора. Это позволяет процессу работать быстро и занимать меньше места в памяти.
         
    На момент импорта данных все таблицы уже имеют первичные (PK) и внешние (FK) ключи. Это помогает проверить правильность вводимых данных. Если объем данных большой, создание ключей замедлит импорт данных, поэтому лучше удалить ключи и создать их заново после импорта.

    Так как проект имеет учебный характер, способ ввода данных был выбран для изучения работы коннектора Psycopg и обработки данных в Pandas.
    
* Альтернативный импорт данных
    
    Альтернативным решением ввода данных является их копирование при помощи функции COPY СУБД Postgres. Такой способ предполагает редактирование данных уже внутри БД и последующую модификацию исходных таблиц. Также возможно редактирование набора данных отдельно до их импорта и импорт в готовые таблицы также при помощи COPY. Вышеприведенные способы импорта следует рассматривать при большом размере исходных файлов.

    
### 2. Source (Postgres) -> Stage (ClickHouse) -> Target (ClickHouse)
В ДАГе №2 сначала производится перенос всех OLTP данных из Source слоя в Staging слой. После этого данные трансформируются и переносятся в Target слой. Для ввода данных из Postgres в Clickhouse используется плагин "clickhouse-plugin" https://github.com/bryzgaloff/airflow-clickhouse-plugin. Он устанавливается в окружение Airflow из requirements на этапе cборки Docker-образа.

Код содержит sql-операторы, которым нужны одни и те же значения параметров. Для удобства форматирования кода используется метод .format.

Визуализация зависимостей задач ДАГа:

<img src="https://github.com/vlkudriashev/data_engineering_project/blob/main/pics/airflow_tasks.jpg" width=50%>

### Описание операторов и задач
#### Задачи  "insert_stg_all_", "insert_tgt_simple_", "insert_tgt_orders" 
При помощи оператора "insert_stg_all" для каждой таблицы из Source слоя динамически создаётся задача по перенос данных в Staging слой без изменений. К данным добавляется поле "insertion_ts", чтобы знать время их загрузки. В дальнейшем это позволит идентифицировать значения для загрузки в Target слой. Данные загружаются в партиции по дням.

В задаче "insert_tgt_simple" выполняется перенос данных таблиц, где не требуется трансформация или изменение данных. В "insert_tgt_orders" производится преобразование  данных таблиц "orders" и "order_items" обратно в массивы для эффективного хранения.
    
#### Задача "insert_tgt_products"(SCD)

Для таблицы с ценами товаров реализуется Slowly Changing Dimensions второго типа (SCD Type 2), так как при построении аналитики обычно требуется цена товара на конкретный момент времени. Реализация SCD позволит отслеживать изменение цены с нужной гранулярностью (раз в день, два раза в день, раз в шесть часов и так далее), если настроить выгрузку данных данные с нужной периодичностью.

Для реализации SCD в таблице "products" в "target_db" вводятся такие поля как "start_ts" и "end_ts", отражающие сроки начала и конца действия записи. Текущему (актуальному) значению мы присваиваем "end_ts" = '2100-01-01 00:00:00', что практически равно максимальной дате в DateTime типе ClickHouse. Прошлые максимальные значения для записей с одним и тем же идентификатором будут позже удалены движком ReplacingMergeTree Clickhouse. На место прошлых "максимумов" записываются значения, где датой окончания является дата начала новой записи с тем же идентификатором. Альтернативой такой замены значений для классических СУБД (например, Postgres) является Upsert (Insert - Update).

Эволюция схемы данных таблицы "products":

<img src="https://github.com/vlkudriashev/data_engineering_project/blob/main/pics/products_tbl_evolution.png" width=50%>
   
Проблему дублирования данных мы решаем использованием движка ReplacingMergeTree. Больше о нём можно можно узнать в статье: [ClickHouse ReplacingMergeTree Explained: The Good, The Bad, and The Ugly | Altinity Blog](https://altinity.com/blog/clickhouse-replacingmergetree-explained-the-good-the-bad-and-the-ugly)
 
В статье приводятся сведения о том, что при использовании параметра "do_not_merge_across_partitions_select_final=1" для таблицы в 900 млн строк можно получить скорость выполнения запросов всего на 30% медленнее, чем без использования FINAL. Известно, что если данные находятся в разных партициях, то замена значений на более новые может не произойти вовсе. В нашем примере данные можно разбить на партиции по дате начала действия записи. Тогда данные об одной записи, подлежащей изменению, всегда (или почти всегда) будут иметь одну дату начала действия записи. В таком случае удаление дубликатов при запросах будет работать и корректно и быстро.

На этом работа по загрузке OLTP данных завершается.
    
### 3. Работа с данными АПИ
В ДАГе №3 тестируется способность pipeline корректно обрабатывать данные API:

#### Таск API в Stage (ClickHouse)
Здесь происходит обращение к запущенному в докере FastAPI приложению (как локальному аналогу удаленного API) и запрашивается некоторое число записей. К записям добавляется идентификатор времени загрузки данных, после чего данные записываются в Staging базу данных в "Nested" формате для их компактного хранения.

#### Таск API (Stage) -> Target (ClickHouse)
В операторе производится преобразование данных из Nested формата в формат обычной строковой записи. Эта операция должна облегчить понимание данных конечными пользователями и позволит удобно производить их дальнейшую обработку. Для записи значений используется коннектор ClickHouse Driver и PythonOperator ДАГа.

 ### Дальнейшие улучшения
1. В некоторых операторах присутствует выборка всех полей таблиц (SELECT *). Явное указание колонок использовалось там, где происходила трансформация данных. Хорошей практикой считается явное задание полей таблиц, с тем, чтобы при изменении схем данных таблиц Pipeline работал стабильно. Явное указание колонок для произвольного числа таблиц можно реализовать, используя конфигурационный файл.
2. На данном этапе, данные об исполнении задача хранятся лишь в Airflow. В дальнейшем можно реализовать запись таких данных во внутреннюю СУБД.
3. Возможно проработать механизм проверки качества вводимых данных и сбора статистики по по записанным значениям.
4. В дальнейшем можно проработать дедупликацию и данных в Target слое с использованием партиций.

### Результаты выполнения проекта
* Разработан Pipeline для управления данными с использованием Airflow
* Реализованы основные принципы работы с данными при построении систем управления данными.
* На практике применены такие понятия теории СУБД как первичные и внешние ключи, ссылочная целостность, нормализация отношений.
* Изучены особенности работы с коннекторами ClickHouse Driver, Psycopg для импорта данных в СУБД
* Изучены основные особенности движков и форматов данных СУБД ClickHouse.
* Реализована концепция медленно меняющихся измерений (SCD) для сохранения истории изменения данных
* Проведена работа по трансформации данных с использованием инструмента Pandas.